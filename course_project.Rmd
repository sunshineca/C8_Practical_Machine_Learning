---
title: "Practical Machine Learning Course Porject"
output: pdf_document
---
## Background
Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset). 


## Abstract 
The goal of this project is to predict the manner in which they did the exercise. This report addresses on the following questions:

- how to built the model;
- how to use cross validation;
- what is the expected out of sample error; 
- why you made the choices you did.

Finally, the prediction model is used to predict 20 different test cases. 

## Data 
The training data for this project are downloaded from the link as below:

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv

The test data are downloaded from the link as below: 

https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

The data for this project come from this source: http://groupware.les.inf.puc-rio.br/har.

```{r, echo=FALSE}
setwd("C:/Jing/ProgramCode/DataScience/C8_Practical_Machine_Learning")
getwd()

library(AppliedPredictiveModeling)
library(caret)
library(rattle)
library(rpart.plot)
library(randomForest)
library(e1071)
```

```{r}
# Download data.
# url_raw_training <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
file_dest_training <- "pml-training.csv"
# download.file(url=url_raw_training, destfile=file_dest_training)
# url_raw_testing <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
file_dest_testing <- "pml-testing.csv"
# download.file(url=url_raw_testing, destfile=file_dest_testing)

# Import the data treating empty values as NA.
df_training <- read.csv(file_dest_training, na.strings=c("NA",""), header=TRUE)
colnames_train <- colnames(df_training)
df_testing <- read.csv(file_dest_testing, na.strings=c("NA",""), header=TRUE)
colnames_test <- colnames(df_testing)

# Verify that the column names (excluding classe and problem_id) are identical in the training and test set.
all.equal(colnames_train[1:length(colnames_train)-1], colnames_test[1:length(colnames_train)-1])
```

# Data Pre-process

There are some columns containing the NA or missing values. These did not contribute well to the prediction.  We chose a feature set that only included complete columns.  

```{r}
# Count the number of non-NAs in each col.
nonNAs <- function(x) {
    as.vector(apply(x, 2, function(x) length(which(!is.na(x)))))
}

# Build vector of missing data or NA columns to drop.
colcnts <- nonNAs(df_training)
drops <- c()
for (cnt in 1:length(colcnts)) {
    if (colcnts[cnt] < nrow(df_training)) {
        drops <- c(drops, colnames_train[cnt])
    }
}

# Drop NA data and the first 7 columns as they're unnecessary for predicting.
df_training <- df_training[,!(names(df_training) %in% drops)]
df_training <- df_training[,8:length(colnames(df_training))]

df_testing <- df_testing[,!(names(df_testing) %in% drops)]
df_testing <- df_testing[,8:length(colnames(df_testing))]
```

# Algorithms

We were provided with a large training set (19,622 entries) and a small testing set (20 entries). Instead of performing the algorithm on the entire training set, as it would be time consuming and wouldn't allow for an attempt on a testing set, I chose to divide the given training set into four roughly equal sets, each of which was then split into a training set (comprising 60% of the entries) and a testing set (comprising 40% of the entries).

```{r}
# Divide the given training set into 4 roughly equal sets.
set.seed(914)
ids_small <- createDataPartition(y=df_training$classe, p=0.25, list=FALSE)
df_small1 <- df_training[ids_small,]
df_remainder <- df_training[-ids_small,]
set.seed(914)
ids_small <- createDataPartition(y=df_remainder$classe, p=0.33, list=FALSE)
df_small2 <- df_remainder[ids_small,]
df_remainder <- df_remainder[-ids_small,]
set.seed(914)
ids_small <- createDataPartition(y=df_remainder$classe, p=0.5, list=FALSE)
df_small3 <- df_remainder[ids_small,]
df_small4 <- df_remainder[-ids_small,]
# Divide each of these 4 sets into training (60%) and test (40%) sets.
set.seed(914)
inTrain <- createDataPartition(y=df_small1$classe, p=0.6, list=FALSE)
df_small_training1 <- df_small1[inTrain,]
df_small_testing1 <- df_small1[-inTrain,]
set.seed(914)
inTrain <- createDataPartition(y=df_small2$classe, p=0.6, list=FALSE)
df_small_training2 <- df_small2[inTrain,]
df_small_testing2 <- df_small2[-inTrain,]
set.seed(914)
inTrain <- createDataPartition(y=df_small3$classe, p=0.6, list=FALSE)
df_small_training3 <- df_small3[inTrain,]
df_small_testing3 <- df_small3[-inTrain,]
set.seed(914)
inTrain <- createDataPartition(y=df_small4$classe, p=0.6, list=FALSE)
df_small_training4 <- df_small4[inTrain,]
df_small_testing4 <- df_small4[-inTrain,]
```

We made use of caret and randomForest model for prediction on the test datasets. Firstly, we train the first subtraining set 1 with both preprocessing and cross validation. 

```{r}
# Train on training set 1 of 4 with both preprocessing and cross validation.
set.seed(914)
modFit <- train(df_small_training1$classe ~ ., method="rf", preProcess=c("center", "scale"), trControl=trainControl(method = "cv", number = 4), data=df_small_training1)
print(modFit, digits=3)
# Run against testing set 1 of 4.
predictions <- predict(modFit, newdata=df_small_testing1)
print(confusionMatrix(predictions, df_small_testing1$classe), digits=4)
# Run against 20 testing set.
print(predict(modFit, newdata=df_testing))
```
We applied both preprocessing and cross validation on other three subdatasets.
```{r}
# Train on training set 2 of 4 with only cross validation.
set.seed(914)
modFit <- train(df_small_training2$classe ~ ., method="rf", preProcess=c("center", "scale"), trControl=trainControl(method = "cv", number = 4), data=df_small_training2)
print(modFit, digits=3)
# Run against testing set 2 of 4.
predictions <- predict(modFit, newdata=df_small_testing2)
print(confusionMatrix(predictions, df_small_testing2$classe), digits=4)
# Run against 20 testing set provided by Professor Leek.
print(predict(modFit, newdata=df_testing))
```

```{r}
# Train on training set 3 of 4 with only cross validation.
set.seed(914)
modFit <- train(df_small_training3$classe ~ ., method="rf", preProcess=c("center", "scale"), trControl=trainControl(method = "cv", number = 4), data=df_small_training3)
print(modFit, digits=3)
# Run against testing set 3 of 4.
predictions <- predict(modFit, newdata=df_small_testing3)
print(confusionMatrix(predictions, df_small_testing3$classe), digits=4)
# Run against 20 testing set provided by Professor Leek.
print(predict(modFit, newdata=df_testing))
```

```{r}
# Train on training set 4 of 4 with only cross validation.
set.seed(914)
modFit <- train(df_small_training4$classe ~ ., method="rf", preProcess=c("center", "scale"), trControl=trainControl(method = "cv", number = 4), data=df_small_training4)
print(modFit, digits=3)
# Run against testing set 4 of 4.
predictions <- predict(modFit, newdata=df_small_testing4)
print(confusionMatrix(predictions, df_small_testing4$classe), digits=4)
# Run against 20 testing set provided by Professor Leek.
print(predict(modFit, newdata=df_testing))
```
## Out of Sample Error

The out of sample error, the error rate after running the predict() function on the 4 testing sets, are listed:

Random Forest (preprocessing and cross validation) Testing Set 1: 1 - 0.9648 = 0.0352
Random Forest (preprocessing and cross validation) Testing Set 2: 1 - 0.9629 = 0.0371 
Random Forest (preprocessing and cross validation) Testing Set 3: 1 - 0.968  = 0.032
Random Forest (preprocessing and cross validation) Testing Set 4: 1 - 0.9639 = 0.0361

Since each testing set is roughly of equal size, I decided to average the out of sample error rates derived by applying the random forest method with both preprocessing and cross validation against test sets 1-4 yielding a predicted out of sample rate of 0.0351.

# CONCLUSION

I received three separate predictions by appling the 4 models against the actual 20 item training set:
                                    
1) Accuracy Rate 0.0352 Predictions:  B A B A A E D B A A B C B A E E A B B B

2) Accuracy Rates 0.0371 Predictions: B A B A A E D D A A B C B A E E A B B B

3) Accuracy Rate 0.032 Predictions:   B A A A A E D B A A B C B A E E A B B B

4) Accuracy Rate 0.0361 Predictions:  B A B A A E D D A A B C B A E E A B B B

Finally, I choose the same two result as the final prediction: B A B A A E D B A A B C B A E E A B B B.

